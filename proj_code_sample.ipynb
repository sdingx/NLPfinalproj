{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Final Project\n",
    "Sam Ding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 7 workers\n"
     ]
    }
   ],
   "source": [
    "# basic data analytics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pickle\n",
    "\n",
    "# nlp modules\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "import multiprocessing\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "import warnings\n",
    "\n",
    "# warnings.simplefilter('once')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "num_processors = multiprocessing.cpu_count()\n",
    "num_processors\n",
    "\n",
    "workers = num_processors-1\n",
    "\n",
    "print(f'Using {workers} workers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# df_news_final_project = pd.read_parquet('https://storage.googleapis.com/msca-bdp-data-open/news_final_project/news_final_project.parquet', engine='pyarrow')\n",
    "# df_news_final_project.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # zero-shot classification\n",
    "# import torch\n",
    "# from transformers import pipeline\n",
    "# classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news_final_project = pd.read_csv('sample_600.csv', index_col=0)\n",
    "# df_news_final_project.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>language</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39396</th>\n",
       "      <td>https://www.wkms.org/npr-news/npr-news/2022-10...</td>\n",
       "      <td>2022-10-10</td>\n",
       "      <td>en</td>\n",
       "      <td>Artificial intelligence could soon diagnose il...</td>\n",
       "      <td>\\n\\nArtificial intelligence could soon diagnos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143316</th>\n",
       "      <td>https://www.wbko.com/prnewswire/2022/08/25/ult...</td>\n",
       "      <td>2022-08-25</td>\n",
       "      <td>en</td>\n",
       "      <td>UltraSight Receives CE Mark for Novel Cardiac ...</td>\n",
       "      <td>UltraSight Receives CE Mark for Novel Cardiac ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      url        date   \n",
       "39396   https://www.wkms.org/npr-news/npr-news/2022-10...  2022-10-10  \\\n",
       "143316  https://www.wbko.com/prnewswire/2022/08/25/ult...  2022-08-25   \n",
       "\n",
       "       language                                              title   \n",
       "39396        en  Artificial intelligence could soon diagnose il...  \\\n",
       "143316       en  UltraSight Receives CE Mark for Novel Cardiac ...   \n",
       "\n",
       "                                                     text  \n",
       "39396   \\n\\nArtificial intelligence could soon diagnos...  \n",
       "143316  UltraSight Receives CE Mark for Novel Cardiac ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news_final_project.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean-up newlines\n",
    "# df_news_final_project['text_clean'] = df_news_final_project['text'].str.replace('\\n', ' ')\n",
    "\n",
    "# clean up tabs\n",
    "df_news_final_project['text_clean'] = df_news_final_project['text'].str.replace('\\t', ' ')\n",
    "\n",
    "# clean up links\n",
    "df_news_final_project['text_clean'] = df_news_final_project['text_clean'].str.replace(r'(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)', '')\n",
    "\n",
    "# clean up remnants of web crawls\n",
    "df_news_final_project['text_clean'] = df_news_final_project['text_clean'].str.replace(r'&#\\d+;', '', case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "notitle = []\n",
    "for i in df_news_final_project.index.values:\n",
    "    notitle.append(df_news_final_project['text_clean'][i].replace(df_news_final_project['title'][i], 'Hahahah 23, 2026')) # replace by this chunk so title can also be split by pattern\n",
    "\n",
    "df_news_final_project['text_notitle'] = notitle\n",
    "\n",
    "# drop everything after string 'for more information'ABC\n",
    "\n",
    "df_news_final_project['text_notitle'] = df_news_final_project['text_notitle'].str.split(r'[F|f]or more information', expand=True)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'(\\w{3,10}\\.*\\s\\d{1,2}\\,*\\s20\\d{2})|(\\d{1,2}\\s\\w{3,10}\\.*\\s20\\d{2})|\\n+'\n",
    "\n",
    "df_news_final_project['split'] = df_news_final_project['text_notitle'].apply(lambda x: re.split(pattern=pattern, string=x))\n",
    "df_news_final_project['split_len'] = df_news_final_project['split'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter df_news_final_project so that it only contains articles that mentions \"artificial intelligence\" or 'data'\n",
    "df_news_final_project = df_news_final_project[df_news_final_project['text_clean'].str.contains('artificial intelligence|data')]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len 1 was 116, after new pattern 78, after new pattern all splitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def get_most_similar(index):\n",
    "    '''\n",
    "    This function takes in an index of the dataframe and returns the most similar text to the title,\n",
    "    filtering out other unnecessary texts.\n",
    "    '''\n",
    "\n",
    "    title = df_news_final_project['title'][index]\n",
    "    texts = df_news_final_project['split'][index]\n",
    "\n",
    "    # print('Original:\\n', texts)\n",
    "\n",
    "    # compare capital letters and periods, drop if there are more capital letters in the texts\n",
    "    texts = [x for x in texts if x != None]\n",
    "\n",
    "    # print('Dropping None:\\n', texts)\n",
    "\n",
    "    # print('Dropping more period than capital:\\n', texts)\n",
    "\n",
    "    # take out texts that are too short\n",
    "    texts = [x for x in texts if len(x) > 150]\n",
    "\n",
    "    # print('Dropping too short:\\n', texts)\n",
    "\n",
    "    # drop texts with Tab patterns\n",
    "    pattern = r'([A-Z][a-z]+(\\n|\\t)+){4}'\n",
    "    texts =  [re.sub(pattern, '', x) for x in texts]\n",
    "\n",
    "    # drop texts with Sign up or email patterns\n",
    "    texts = [x for x in texts if re.search(r'([S|s]ign up)|([E|e]mail)', x) == None]\n",
    "\n",
    "    # print('Dropping Tab patterns:\\n', texts)\n",
    "\n",
    "    # keep texts with small letter patterns\n",
    "    pattern = r'([a-z]+\\s){4}'\n",
    "    texts = [x for x in texts if re.search(pattern, x) != None]\n",
    "\n",
    "    # print('Keeping small letter patterns:\\n', texts)\n",
    "\n",
    "\n",
    "    # return NA if there are no text splits left\n",
    "    if len(texts) == 0:\n",
    "        return np.nan\n",
    "\n",
    "    # Tokenize the title and texts\n",
    "    tokenized_title = nltk.word_tokenize(title.lower())\n",
    "    tokenized_texts = [nltk.word_tokenize(text.lower()) for text in texts]\n",
    "\n",
    "    # Convert the tokenized texts to strings\n",
    "    text_strings = [' '.join(tokens) for tokens in tokenized_texts]\n",
    "\n",
    "    # print('Tokenized texts:\\n', len(text_strings))\n",
    "\n",
    "    index_to_return = 0\n",
    "\n",
    "    # Create a TF-IDF vectorizer and fit it to the text strings\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(text_strings)\n",
    "        # print(tfidf_matrix.shape)\n",
    "\n",
    "    # Compute the cosine similarity between the title and each text\n",
    "    title_vector = vectorizer.transform([' '.join(tokenized_title)])\n",
    "    similarity_scores = cosine_similarity(title_vector, tfidf_matrix)\n",
    "\n",
    "    most_similar_index = similarity_scores.argmax()\n",
    "        \n",
    "    most_similar_score = similarity_scores.max()\n",
    "    index_to_return = most_similar_index\n",
    "        \n",
    "    list_articles = []\n",
    "    list_scores = []\n",
    "\n",
    "    for i in range(0,len(similarity_scores[0])):\n",
    "        if similarity_scores[0][i] >= 0.07:\n",
    "            list_articles.append(text_strings[i])\n",
    "            list_scores.append(similarity_scores[0][i])\n",
    "        # print(len(text_strings), len(similarity_scores[0]))\n",
    "                \n",
    "        # if len(text_strings) > 1:\n",
    "        #     second_most_similar_index = similarity_scores.argsort()[0][-2]\n",
    "        #     # print(similarity_scores)\n",
    "        #     second_most_similar_score = similarity_scores[0][second_most_similar_index]\n",
    "        #     if most_similar_score > 0.2:\n",
    "        #         index_to_return = second_most_similar_index\n",
    "        # # return texts[index_to_return]\n",
    "        # return \n",
    "\n",
    "    # except:\n",
    "    #     pass\n",
    "    \n",
    "\n",
    "    return ' '.join(list_articles)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(574, 9)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar = []\n",
    "\n",
    "for i in df_news_final_project.index.values:\n",
    "    most_similar.append(get_most_similar(i))\n",
    "\n",
    "df_news_final_project['text_clean'] = most_similar\n",
    "\n",
    "df_news_final_project.dropna(subset=['text_clean'], inplace=True)\n",
    "\n",
    "df_news_final_project.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       39396\n",
       "1      143316\n",
       "2      100092\n",
       "3       21501\n",
       "4       79650\n",
       "        ...  \n",
       "569     12183\n",
       "570     69983\n",
       "571      9710\n",
       "572     45140\n",
       "573    150157\n",
       "Length: 574, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(df_news_final_project.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(indices):\n",
    "    entities = []\n",
    "    batch_size = len(indices)\n",
    "    texts = df_news_final_project.loc[indices, 'text_clean']\n",
    "    docs = nlp.pipe(texts, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "\n",
    "    for i in range(0,batch_size):\n",
    "        index = indices[i]\n",
    "        doc = next(docs)\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in ['PERSON', 'ORG', 'PRODUCT', 'EVENT']:\n",
    "                entities.append((index, ent.text, ent.label_))\n",
    "\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "from pandarallel import pandarallel\n",
    "\n",
    "# Initialize Pandarallel\n",
    "pandarallel.initialize()\n",
    "# divide index into batches\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "index_batches = np.array_split(df_news_final_project.index.values, 100)\n",
    "\n",
    "# apply function to each batch\n",
    "\n",
    "ner_df = pd.Series(index_batches).parallel_apply(extract_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe from the list of entities\n",
    "ner_df = pd.DataFrame([item for sublist in ner_df for item in sublist], columns=['index', 'entity', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entity</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>chatgpt</th>\n",
       "      <td>258</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft</th>\n",
       "      <td>171</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>covid-19</th>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ibm</th>\n",
       "      <td>76</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https</th>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gray media group</th>\n",
       "      <td>68</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gray television , inc</th>\n",
       "      <td>68</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>google</th>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amazon</th>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fda</th>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baidu</th>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dr.</th>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ml</th>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>don</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>samsung</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edgecortix</th>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ap</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vermillio</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sam altman</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deepbrain ai</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intel</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gray media group , inc.</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lunit</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gdpr</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spotify</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>govee</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nyse</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>descriptiona gray media group , inc.</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>naver</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>android</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>global x robotics &amp;</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>david</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mit</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>darktrace</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hailo</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>congress</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lg</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ford</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>air india</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flytxt</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agolikecommentsharesilicon valley</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bayanat</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cltv</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kumo</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agot</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>serviceprivacy policyeeo statementadvertisinga gray media group , inc.</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kakaclo</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>biden</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reuters</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concertai</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    index  label\n",
       "entity                                                          \n",
       "chatgpt                                               258    258\n",
       "microsoft                                             171    171\n",
       "covid-19                                               93     93\n",
       "ibm                                                    76     76\n",
       "https                                                  69     69\n",
       "gray media group                                       68     68\n",
       "gray television , inc                                  68     68\n",
       "google                                                 65     65\n",
       "amazon                                                 47     47\n",
       "fda                                                    23     23\n",
       "baidu                                                  23     23\n",
       "dr.                                                    23     23\n",
       "ml                                                     22     22\n",
       "don                                                    18     18\n",
       "samsung                                                18     18\n",
       "edgecortix                                             17     17\n",
       "ap                                                     16     16\n",
       "vermillio                                              16     16\n",
       "sam altman                                             16     16\n",
       "deepbrain ai                                           16     16\n",
       "intel                                                  15     15\n",
       "gray media group , inc.                                15     15\n",
       "lunit                                                  15     15\n",
       "gdpr                                                   15     15\n",
       "spotify                                                14     14\n",
       "govee                                                  14     14\n",
       "nyse                                                   14     14\n",
       "descriptiona gray media group , inc.                   14     14\n",
       "naver                                                  14     14\n",
       "android                                                13     13\n",
       "global x robotics &                                    13     13\n",
       "david                                                  13     13\n",
       "mit                                                    13     13\n",
       "darktrace                                              12     12\n",
       "hailo                                                  11     11\n",
       "congress                                               11     11\n",
       "lg                                                     11     11\n",
       "ford                                                   11     11\n",
       "air india                                              11     11\n",
       "flytxt                                                 11     11\n",
       "agolikecommentsharesilicon valley                      11     11\n",
       "bayanat                                                11     11\n",
       "cltv                                                   10     10\n",
       "kumo                                                   10     10\n",
       "agot                                                   10     10\n",
       "serviceprivacy policyeeo statementadvertisinga ...      9      9\n",
       "kakaclo                                                 9      9\n",
       "biden                                                   9      9\n",
       "reuters                                                 9      9\n",
       "concertai                                               9      9"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# search for specific entities with chatgpt\n",
    "ner_df.groupby('entity').count().sort_values(by='index', ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>entity</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39396</td>\n",
       "      <td>allison longyael bensoussan</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39396</td>\n",
       "      <td>the usf health 's</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39396</td>\n",
       "      <td>dr.</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39396</td>\n",
       "      <td>yael bensoussan</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39396</td>\n",
       "      <td>the university of south florida 's</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7497</th>\n",
       "      <td>150157</td>\n",
       "      <td>articlesafter</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7498</th>\n",
       "      <td>150157</td>\n",
       "      <td>farjayhawks</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7499</th>\n",
       "      <td>150157</td>\n",
       "      <td>mountaineersgradey dick</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7500</th>\n",
       "      <td>150157</td>\n",
       "      <td>winkansas softball</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7501</th>\n",
       "      <td>150157</td>\n",
       "      <td>mccullar</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7502 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                              entity   label\n",
       "0      39396         allison longyael bensoussan  PERSON\n",
       "1      39396                   the usf health 's     ORG\n",
       "2      39396                                 dr.  PERSON\n",
       "3      39396                     yael bensoussan  PERSON\n",
       "4      39396  the university of south florida 's     ORG\n",
       "...      ...                                 ...     ...\n",
       "7497  150157                       articlesafter     ORG\n",
       "7498  150157                         farjayhawks     ORG\n",
       "7499  150157             mountaineersgradey dick     ORG\n",
       "7500  150157                  winkansas softball     ORG\n",
       "7501  150157                            mccullar  PERSON\n",
       "\n",
       "[7502 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>entity</th>\n",
       "      <th>label</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39396</td>\n",
       "      <td>allison longyael bensoussan</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>2022-10-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39396</td>\n",
       "      <td>the usf health 's</td>\n",
       "      <td>ORG</td>\n",
       "      <td>2022-10-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39396</td>\n",
       "      <td>dr.</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>2022-10-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39396</td>\n",
       "      <td>yael bensoussan</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>2022-10-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39396</td>\n",
       "      <td>the university of south florida 's</td>\n",
       "      <td>ORG</td>\n",
       "      <td>2022-10-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7497</th>\n",
       "      <td>150157</td>\n",
       "      <td>articlesafter</td>\n",
       "      <td>ORG</td>\n",
       "      <td>2023-02-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7498</th>\n",
       "      <td>150157</td>\n",
       "      <td>farjayhawks</td>\n",
       "      <td>ORG</td>\n",
       "      <td>2023-02-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7499</th>\n",
       "      <td>150157</td>\n",
       "      <td>mountaineersgradey dick</td>\n",
       "      <td>ORG</td>\n",
       "      <td>2023-02-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7500</th>\n",
       "      <td>150157</td>\n",
       "      <td>winkansas softball</td>\n",
       "      <td>ORG</td>\n",
       "      <td>2023-02-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7501</th>\n",
       "      <td>150157</td>\n",
       "      <td>mccullar</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>2023-02-27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7502 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                              entity   label        date\n",
       "0      39396         allison longyael bensoussan  PERSON  2022-10-10\n",
       "1      39396                   the usf health 's     ORG  2022-10-10\n",
       "2      39396                                 dr.  PERSON  2022-10-10\n",
       "3      39396                     yael bensoussan  PERSON  2022-10-10\n",
       "4      39396  the university of south florida 's     ORG  2022-10-10\n",
       "...      ...                                 ...     ...         ...\n",
       "7497  150157                       articlesafter     ORG  2023-02-27\n",
       "7498  150157                         farjayhawks     ORG  2023-02-27\n",
       "7499  150157             mountaineersgradey dick     ORG  2023-02-27\n",
       "7500  150157                  winkansas softball     ORG  2023-02-27\n",
       "7501  150157                            mccullar  PERSON  2023-02-27\n",
       "\n",
       "[7502 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge with ner_df\n",
    "ner_df.merge(df_news_final_project['date'], left_on='index', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>entity</th>\n",
       "      <th>label</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39396</td>\n",
       "      <td>allison longyael bensoussan</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>2022-10-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39396</td>\n",
       "      <td>the usf health 's</td>\n",
       "      <td>ORG</td>\n",
       "      <td>2022-10-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39396</td>\n",
       "      <td>dr.</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>2022-10-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39396</td>\n",
       "      <td>yael bensoussan</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>2022-10-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39396</td>\n",
       "      <td>the university of south florida 's</td>\n",
       "      <td>ORG</td>\n",
       "      <td>2022-10-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7497</th>\n",
       "      <td>150157</td>\n",
       "      <td>articlesafter</td>\n",
       "      <td>ORG</td>\n",
       "      <td>2023-02-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7498</th>\n",
       "      <td>150157</td>\n",
       "      <td>farjayhawks</td>\n",
       "      <td>ORG</td>\n",
       "      <td>2023-02-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7499</th>\n",
       "      <td>150157</td>\n",
       "      <td>mountaineersgradey dick</td>\n",
       "      <td>ORG</td>\n",
       "      <td>2023-02-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7500</th>\n",
       "      <td>150157</td>\n",
       "      <td>winkansas softball</td>\n",
       "      <td>ORG</td>\n",
       "      <td>2023-02-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7501</th>\n",
       "      <td>150157</td>\n",
       "      <td>mccullar</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>2023-02-27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7488 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                              entity   label        date\n",
       "0      39396         allison longyael bensoussan  PERSON  2022-10-10\n",
       "1      39396                   the usf health 's     ORG  2022-10-10\n",
       "2      39396                                 dr.  PERSON  2022-10-10\n",
       "3      39396                     yael bensoussan  PERSON  2022-10-10\n",
       "4      39396  the university of south florida 's     ORG  2022-10-10\n",
       "...      ...                                 ...     ...         ...\n",
       "7497  150157                       articlesafter     ORG  2023-02-27\n",
       "7498  150157                         farjayhawks     ORG  2023-02-27\n",
       "7499  150157             mountaineersgradey dick     ORG  2023-02-27\n",
       "7500  150157                  winkansas softball     ORG  2023-02-27\n",
       "7501  150157                            mccullar  PERSON  2023-02-27\n",
       "\n",
       "[7488 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_df[ner_df['label'].isin(['PERSON', 'ORG', 'PRODUCT'])].merge(df_news_final_project['date'], left_on='index', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([ 39396, 143316, 100092,  21501,  79650,  83882, 146180,  45208, 139248,\n",
       "         9867,\n",
       "       ...\n",
       "       129684, 102030, 130396, 117860, 114660,  12183,  69983,   9710,  45140,\n",
       "       150157],\n",
       "      dtype='int64', length=574)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news_final_project.index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>entity</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39396</td>\n",
       "      <td>allison longyael bensoussan</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39396</td>\n",
       "      <td>the usf health 's</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39396</td>\n",
       "      <td>dr.</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39396</td>\n",
       "      <td>yael bensoussan</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39396</td>\n",
       "      <td>the university of south florida 's</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7497</th>\n",
       "      <td>150157</td>\n",
       "      <td>articlesafter</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7498</th>\n",
       "      <td>150157</td>\n",
       "      <td>farjayhawks</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7499</th>\n",
       "      <td>150157</td>\n",
       "      <td>mountaineersgradey dick</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7500</th>\n",
       "      <td>150157</td>\n",
       "      <td>winkansas softball</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7501</th>\n",
       "      <td>150157</td>\n",
       "      <td>mccullar</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7502 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                              entity   label\n",
       "0      39396         allison longyael bensoussan  PERSON\n",
       "1      39396                   the usf health 's     ORG\n",
       "2      39396                                 dr.  PERSON\n",
       "3      39396                     yael bensoussan  PERSON\n",
       "4      39396  the university of south florida 's     ORG\n",
       "...      ...                                 ...     ...\n",
       "7497  150157                       articlesafter     ORG\n",
       "7498  150157                         farjayhawks     ORG\n",
       "7499  150157             mountaineersgradey dick     ORG\n",
       "7500  150157                  winkansas softball     ORG\n",
       "7501  150157                            mccullar  PERSON\n",
       "\n",
       "[7502 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(574, 9)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news_final_project.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "632"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"© 2023 ideastream public media1375 euclid avenue , cleveland , ohio 44115 ( 216 ) 916-6100 | ( 877 ) 399-3307wksu is a public media service licensed to kent state university and operated by ideastream public media . the tech world enthusiastic about the possibilities of artificial intelligence , but where does that leave meta 's plans for the metaverse ? some large companies are already dialing back their plans . © 2023 ideastream public media1375 euclid avenue , cleveland , ohio 44115 ( 216 ) 916-6100 | ( 877 ) 399-3307wksu is a public media service licensed to kent state university and operated by ideastream public media .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop text clean that are less than 700 characters\n",
    "df_news_final_project = df_news_final_project[df_news_final_project['text_clean'].apply(lambda x: len(x)) > 700]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have the cleaned text for downstream analysis. Currently there are 587 documents that are not null and have the mentions of those words in Data Science."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/sding/.env/lib/python3.9/site-packages (from vaderSentiment) (2.28.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sding/.env/lib/python3.9/site-packages (from requests->vaderSentiment) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sding/.env/lib/python3.9/site-packages (from requests->vaderSentiment) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/sding/.env/lib/python3.9/site-packages (from requests->vaderSentiment) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sding/.env/lib/python3.9/site-packages (from requests->vaderSentiment) (2022.12.7)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targeted sentiment analysis\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.11, 'neu': 0.748, 'pos': 0.142, 'compound': 0.6798}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = df_news_final_project['text_clean'].iloc[3]\n",
    "sia.polarity_scores(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the campaign ai now recruits more intelligently , and transfers units to threatened theaters actively .',\n",
       " 'it is more active in construction and management of its fleets , and will also consider naval invasions against poorly protected shores .',\n",
       " 'capital city defense behavior , defensive operations , weapons production and distribution among the troops , replacing commanders with better ones , and organizing early armies into grand armies with corps organization are also among the upgrades .',\n",
       " 'the battle ai handles its troops better .',\n",
       " 'in attack , it will bombard enemy positions with its artillery while waiting for more troops to arrive , before moving in to close combat , if required .',\n",
       " \"it is more active in defensive battles to counter player 's flanking and encirclement movements with timely withdrawals , while also considering keeping a reserve to counter player 's maneuvers .\",\n",
       " \"all in all , there will be less confusion in the ai 's ranks during battles .\",\n",
       " 'along comes also a long list of bug fixes , balancing , ui and other improvements , including orderly withdrawal from campaign battles if the losing army is not broken .',\n",
       " \"while continuing to improve the game with the usual bug fixes and other minor improvements , the next patch 1.05 will have its focus in campaign economy , and player 's controls in influencing it .\"]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract each sentence\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(t)\n",
    "\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ktrain.text.sentiment import SentimentAnalyzer\n",
    "classifier = SentimentAnalyzer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POSITIVE'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(classifier.predict(sent).keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = sents.pop(0)\n",
    "# take the latter half of the text by length\n",
    "text = text[len(text[0])//2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" and gradient ventures ( google 's ai-focused fund ) . the funding will drive further development and commercialization of rad ai omni and rad ai continuity , the company 's first core offerings on its ai platform , and advance rad ai 's mission to empower radiologists with ai — saving them time , reducing burnout , and helping to improve the quality of patient care.rad ai logohow rad ai helps radiologists and improves patient carefounded in 2018 , rad ai has seen rapid adoption of its ai platform , and is already in use at 7 of the 10 largest private radiology practices in the united states .\""
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the 50 characters to the end\n",
    "item[-600:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ktrain.text.sentiment import SentimentAnalyzer\n",
    "from textblob import TextBlob\n",
    "classifier = SentimentAnalyzer()\n",
    "\n",
    "def detect_sentiment(indices):\n",
    "    sample = df_news_final_project.loc[indices, 'text_clean']\n",
    "    sentiments = []\n",
    "    batch_size = len(indices)\n",
    "    \n",
    "    for i in range(0,batch_size):\n",
    "        # print('a')\n",
    "        index = list(indices)[i]\n",
    "        # print('b')\n",
    "        sents = TextBlob(df_news_final_project['text_clean'][index]).sentences\n",
    "        sents = [sentence.raw for sentence in sents]\n",
    "        # print('c')\n",
    "        item = ' '.join(sents[:2])\n",
    "        # print('d')\n",
    "        # take the bottom 500 characters\n",
    "        item = item[-500:]\n",
    "        # print('e')\n",
    "        # print(item)\n",
    "        sentiments.append(list(classifier.predict(item).keys())[0])\n",
    "        # print('finished index', index)\n",
    "        \n",
    "    \n",
    "    return sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['NEUTRAL', 'POSITIVE', 'POSITIVE', 'POSITIVE', 'NEUTRAL'], ['POSITIVE', 'POSITIVE', 'POSITIVE', 'POSITIVE', 'POSITIVE'], ['NEUTRAL', 'NEGATIVE', 'POSITIVE', 'NEUTRAL', 'POSITIVE'], ['NEUTRAL', 'NEUTRAL', 'NEUTRAL', 'NEUTRAL', 'NEUTRAL'], ['POSITIVE', 'POSITIVE', 'NEUTRAL', 'NEUTRAL', 'NEUTRAL'], ['NEUTRAL', 'POSITIVE', 'POSITIVE', 'POSITIVE', 'POSITIVE'], ['POSITIVE', 'NEUTRAL', 'NEUTRAL', 'POSITIVE', 'NEUTRAL'], ['POSITIVE', 'NEGATIVE', 'NEUTRAL', 'POSITIVE', 'NEUTRAL'], ['POSITIVE', 'NEGATIVE', 'POSITIVE', 'POSITIVE', 'NEUTRAL'], ['NEUTRAL', 'NEUTRAL', 'NEUTRAL', 'POSITIVE', 'NEUTRAL']]\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "def parallel_detect_sentiment(indices):\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = executor.map(detect_sentiment, indices)\n",
    "    return list(results)\n",
    "\n",
    "# Example usage:\n",
    "indices = np.array_split(df_news_final_project.index.values, 100)[:10]\n",
    "sentiments = parallel_detect_sentiment(indices)\n",
    "print(sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NEUTRAL']"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(classifier.predict(item).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NEUTRAL', 'POSITIVE', 'POSITIVE', 'POSITIVE', 'NEUTRAL']"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_sentiment(np.array([ 39396, 143316, 100092,  79650,  83882]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mohammad Hosseini: Should we bring AI into hospitals? Let’s find the middle ground'"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news_final_project['title'][83882]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 39396, 143316, 100092,  79650,  83882]),\n",
       " array([146180,  45208, 139248,   9867, 151392])]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_split(df_news_final_project.index.values, 100)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.32 s, sys: 535 ms, total: 1.86 s\n",
      "Wall time: 1.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pandarallel import pandarallel\n",
    "import time\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "# Initialize Pandarallel\n",
    "\n",
    "classifier = SentimentAnalyzer()\n",
    "\n",
    "# divide index into batches\n",
    "\n",
    "index_batches = np.array_split(df_news_final_project.index.values, 100)[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 39396, 143316, 100092,  79650,  83882]),\n",
       " array([146180,  45208, 139248,   9867, 151392]),\n",
       " array([134684,  36669, 131661,  85628, 150842]),\n",
       " array([ 66510,  53116, 105431, 147677,  47349]),\n",
       " array([ 64127,   3009, 196728,  80755, 108350]),\n",
       " array([ 18977, 190443,  79302,  28138,  90070]),\n",
       " array([  9097,  97881,  53612, 157201,  26861]),\n",
       " array([116294, 157031,  41667, 165204,  62685]),\n",
       " array([154932, 142833,  66542, 106304,  15151]),\n",
       " array([185684, 153973, 130908,  31550,  96694])]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sding/.env/lib/python3.9/site-packages/ktrain/imports.py:59: UserWarning: TensorFlow is not installed and will be needed if training neural networks, but non-TensorFlow features in ktrain can still be used. See https://github.com/amaiya/ktrain/blob/master/README.md\n",
      "  warnings.warn(TF_WARNING)\n",
      "/Users/sding/.env/lib/python3.9/site-packages/ktrain/imports.py:59: UserWarning: TensorFlow is not installed and will be needed if training neural networks, but non-TensorFlow features in ktrain can still be used. See https://github.com/amaiya/ktrain/blob/master/README.md\n",
      "  warnings.warn(TF_WARNING)\n",
      "/Users/sding/.env/lib/python3.9/site-packages/ktrain/imports.py:59: UserWarning: TensorFlow is not installed and will be needed if training neural networks, but non-TensorFlow features in ktrain can still be used. See https://github.com/amaiya/ktrain/blob/master/README.md\n",
      "  warnings.warn(TF_WARNING)\n",
      "/Users/sding/.env/lib/python3.9/site-packages/ktrain/imports.py:59: UserWarning: TensorFlow is not installed and will be needed if training neural networks, but non-TensorFlow features in ktrain can still be used. See https://github.com/amaiya/ktrain/blob/master/README.md\n",
      "  warnings.warn(TF_WARNING)\n",
      "/Users/sding/.env/lib/python3.9/site-packages/ktrain/imports.py:59: UserWarning: TensorFlow is not installed and will be needed if training neural networks, but non-TensorFlow features in ktrain can still be used. See https://github.com/amaiya/ktrain/blob/master/README.md\n",
      "  warnings.warn(TF_WARNING)\n",
      "/Users/sding/.env/lib/python3.9/site-packages/ktrain/imports.py:59: UserWarning: TensorFlow is not installed and will be needed if training neural networks, but non-TensorFlow features in ktrain can still be used. See https://github.com/amaiya/ktrain/blob/master/README.md\n",
      "  warnings.warn(TF_WARNING)\n",
      "/Users/sding/.env/lib/python3.9/site-packages/ktrain/imports.py:59: UserWarning: TensorFlow is not installed and will be needed if training neural networks, but non-TensorFlow features in ktrain can still be used. See https://github.com/amaiya/ktrain/blob/master/README.md\n",
      "  warnings.warn(TF_WARNING)\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "def parallel_detect_sentiment(index_batches):\n",
    "    sent_df = Parallel(n_jobs=7)(delayed(detect_sentiment)(batch) for batch in index_batches)\n",
    "    return sent_df\n",
    "\n",
    "sent_df = parallel_detect_sentiment(index_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liu jin , 31 , has never learned to paint . this “ painting ” was actually generated by him using software – ai painting websites and tools currently popular on social media have the ability to “ turn a sentence into a painting ” . how does ai paint ?\n",
      "s vice president of engineering . sudeep is an accomplished technology leader with over 18 years of experience in the computer vision industry . in his new role he will be located in bengaluru , india and focused on building the next generation of products and services for the artificial intelligence industry.throughout his career sudeep focused on building computational imaging products and solutions at the intersection of computer vision , imaging , edge computing and autonomous capabilities .\n",
      " addition to dayton gastroenterology , a first for independent practices in dayton , stems from the physicians ' commitment to the continuous elevation of quality of care provided to gi patients throughout the greater dayton region . dr. appalaneni explained , `` we have partnered with medtronic and integrated the gi genius ai technology into our everyday colonoscopy procedures at all our locations to potentially increase the adenoma detection rate which decreases the incidence of colon cancer .\n",
      "ement to regulate military ai . the pilot for a four day work week that involved 3,000 employees across 61 british companies is now over and the results are worth thinking about , especially for companies struggling to attract and retain high value employees . the results of the trial consolidate the findings from previous research from 4 day week global , which found that 63 per cent of businesses surveyed found it easier to attract and retain employees after switching to a four-day work week .\n",
      "ng software to make the late chef anthony bourdain say words he never spoke has drawn criticism amid ethical concerns about the use of the powerful technology . the movie roadrunner : a film about anthony bourdain appeared in cinemas friday and mostly features real footage of the beloved celebrity chef and globe-trotting television host before he died in 2018. but its director , morgan neville , told the new yorker that a snippet of dialogue was created using artificial intelligence technology .\n",
      "european scientists working in optoelectronics have created a biocompatible artificial intelligence ( ai ) platform that may enable a novel way for the early detection , monitoring , and treatment of diseases and medical conditions . in current biocompatible organic materials for electronics , namely organic electrochemical transistors ( oects ) , the processing of the information is somewhat inadequate . in efforts to solve this , the scientists took a brain-inspired framework for computation .\n",
      "washington ( ap ) president joe biden met tuesday with his advisory board on science and technology about the risks and opportunities that rapid advances in artificial intelligence development pose to individual users and national security . ai can help address some of the toughest challenges like disease and climate change , but it also needs to address potential risks to society , the economy and national security , biden told the group . rice field .\n",
      "is of products/applications on a regional basis . browse full research report at https : //www.crystalmarketreport.com/global-artificial-intelligence-in-computer-networks-market-size-status-and-forecast-2019-2025 in general , the application of artificial intelligence in computer network technology mainly includes network management , security management , and artificial intelligence.therefore , artificial intelligence is used in computer network technology to meet data processing requirements .\n",
      "london ( ap ) — the company behind chatgpt will propose measures to resolve data privacy concerns that sparked a temporary italian ban on the artificial intelligence chatbot , regulators said thursday . in a video call late wednesday between the watchdog 's commissioners and openai executives including ceo sam altman , the company promised to set out measures to address the concerns . those remedies have not been detailed .\n",
      " notably did not set out specific enforcement actions , but instead was intended as a call to action for the u.s. government to safeguard digital and civil rights in an ai-fueled world.biden 's council , known as pcast , is composed of science , engineering , technology and medical experts and is co-chaired by the cabinet-ranked director of the white house office of science and technology policy , arati prabhakar.asked if ai is dangerous , biden said tuesday , “ it remains to be seen . could be.\n"
     ]
    }
   ],
   "source": [
    "sample = list(df_news_final_project.sample(10)['text_clean'])\n",
    "\n",
    "new_sample = []\n",
    "\n",
    "for item in sample:\n",
    "    sents = nltk.sent_tokenize(item)\n",
    "    item = ' '.join(sents[:3])\n",
    "    # take the bottom 500 characters\n",
    "    item = item[-500:]\n",
    "    print(item)\n",
    "    new_sample.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liu jin , 31 , has never learned to paint . this “ painting ” was actually generated by him using software – ai painting websites and tools currently popular on social media have the ability to “ turn a sentence into a painting ” . how does ai paint ?\n",
      "s vice president of engineering . sudeep is an accomplished technology leader with over 18 years of experience in the computer vision industry . in his new role he will be located in bengaluru , india and focused on building the next generation of products and services for the artificial intelligence industry.throughout his career sudeep focused on building computational imaging products and solutions at the intersection of computer vision , imaging , edge computing and autonomous capabilities .\n",
      " addition to dayton gastroenterology , a first for independent practices in dayton , stems from the physicians ' commitment to the continuous elevation of quality of care provided to gi patients throughout the greater dayton region . dr. appalaneni explained , `` we have partnered with medtronic and integrated the gi genius ai technology into our everyday colonoscopy procedures at all our locations to potentially increase the adenoma detection rate which decreases the incidence of colon cancer .\n",
      "ement to regulate military ai . the pilot for a four day work week that involved 3,000 employees across 61 british companies is now over and the results are worth thinking about , especially for companies struggling to attract and retain high value employees . the results of the trial consolidate the findings from previous research from 4 day week global , which found that 63 per cent of businesses surveyed found it easier to attract and retain employees after switching to a four-day work week .\n",
      "ng software to make the late chef anthony bourdain say words he never spoke has drawn criticism amid ethical concerns about the use of the powerful technology . the movie roadrunner : a film about anthony bourdain appeared in cinemas friday and mostly features real footage of the beloved celebrity chef and globe-trotting television host before he died in 2018. but its director , morgan neville , told the new yorker that a snippet of dialogue was created using artificial intelligence technology .\n",
      "european scientists working in optoelectronics have created a biocompatible artificial intelligence ( ai ) platform that may enable a novel way for the early detection , monitoring , and treatment of diseases and medical conditions . in current biocompatible organic materials for electronics , namely organic electrochemical transistors ( oects ) , the processing of the information is somewhat inadequate . in efforts to solve this , the scientists took a brain-inspired framework for computation .\n",
      "washington ( ap ) president joe biden met tuesday with his advisory board on science and technology about the risks and opportunities that rapid advances in artificial intelligence development pose to individual users and national security . ai can help address some of the toughest challenges like disease and climate change , but it also needs to address potential risks to society , the economy and national security , biden told the group . rice field .\n",
      "is of products/applications on a regional basis . browse full research report at https : //www.crystalmarketreport.com/global-artificial-intelligence-in-computer-networks-market-size-status-and-forecast-2019-2025 in general , the application of artificial intelligence in computer network technology mainly includes network management , security management , and artificial intelligence.therefore , artificial intelligence is used in computer network technology to meet data processing requirements .\n",
      "london ( ap ) — the company behind chatgpt will propose measures to resolve data privacy concerns that sparked a temporary italian ban on the artificial intelligence chatbot , regulators said thursday . in a video call late wednesday between the watchdog 's commissioners and openai executives including ceo sam altman , the company promised to set out measures to address the concerns . those remedies have not been detailed .\n",
      " notably did not set out specific enforcement actions , but instead was intended as a call to action for the u.s. government to safeguard digital and civil rights in an ai-fueled world.biden 's council , known as pcast , is composed of science , engineering , technology and medical experts and is co-chaired by the cabinet-ranked director of the white house office of science and technology policy , arati prabhakar.asked if ai is dangerous , biden said tuesday , “ it remains to be seen . could be.\n"
     ]
    }
   ],
   "source": [
    "sentiments = []\n",
    "for sent in new_sample:\n",
    "    sentiments.append(list(classifier.predict(sent).keys())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NEUTRAL',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEUTRAL',\n",
       " 'NEUTRAL',\n",
       " 'NEUTRAL',\n",
       " 'NEUTRAL',\n",
       " 'NEUTRAL',\n",
       " 'NEUTRAL']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"liu jin , 31 , has never learned to paint . this “ painting ” was actually generated by him using software – ai painting websites and tools currently popular on social media have the ability to “ turn a sentence into a painting ” . how does ai paint ? the reporter tried the overseas website dreamstudio . after adjusting the frame size and other information , the reporter entered keywords such as “ rose ” , “ clouds ” and “ river ” in the blank column of “ dream ” , and used “ oil painting ” as the style . after waiting for more than ten seconds , the ai \\u200b\\u200b\\u200b\\u200bgenerated a picture landscape oil painting in traditional style : pink roses grow on the banks of the river , and clouds under the setting sun are reflected in the river . then the reporter tried the mobile phone software ai dream generator . in addition to describing the picture scene , this mobile phone software also has different styles such as chinese style , cyberpunk style , oil painting style , dark style , etc . it can even simulate the styles of artists such as van gogh , monet , mucha , and picasso . after inputting “ airship in the dark ” , the reporter chose cyberpunk style , and then generated a picture with oil painting style : the object in the picture that looks like a ship and a castle is breaking through the haze , but the background is not like the sky , more like a night street . the generation time of the two works is only ten seconds . the champion work “ space opera ” is signed “ jason allen via midjourney ” – jason allen ( jason allen ) is the ceo of a game company , and midjourney is an ai painting software . jason allen explained that as a “ creator ” , he spent a month constantly revising keywords to make midjourney understand his meaning as accurately as possible , and generated hundreds of works , and finally selected three images for adjustment , the one that participated in the competition was formed , but the voice of opposition was still high . the reason why the author of the ai \\u200b\\u200bpainting is important is that it involves a series of copyright attributions and is also closely related to subsequent development . on wenxin·yige ’ s webpage , a large number of works generated by users using the website system are displayed , and products such as mobile phone cases , mugs , canvas bags and other products using these images are simulated , but the above is marked “ this painting is not for commercial use ” . . see also un general assembly expected to pass resolution condemning russia 's invasion of ukraine and calling for immediate troop withdrawal “ compared to the real art practitioners , from getting inspiration to creating real objects , i didn ’ t even say a word coherently to generate such works . to say that i am the author of this painting , i can ’ t afford it. ” in the circle , liu jin repeatedly explained that the painting was “ generated with ai painting software ” rather than “ created ” by himself . so can ai be considered an author ? in the opinion of the legal profession , this can not be determined at present . article 2 of the copyright law stipulates that “ the works of chinese citizens , legal persons or unincorporated organizations , whether published or not , shall enjoy copyright in accordance with this law. ” some people in the legal profession believe : “ this means that copyright can only be enjoyed by civil subjects , but artificial intelligence does not have the capacity of civil subjects , so ai itself can not be the author of the work . ” according to reports , the basic technology of ai painting is the same as other artificial intelligence , which is to allow the system to deeply learn human works , and to achieve imitation and creation after absorbing a large amount of data and analyzing it . it is this kind of learning and imitation that makes many video art practitioners worry . “ first of all , are the sources and data of artificial intelligence learning legal ? ” said wu xinwei , who is engaged in character creation in an animation company . to solve the problem of plagiarism in reality , we need to look for many details such as composition , image , original elements , etc . “ we sometimes even in the process of creation . hiding some details , some people copied it because they didn ’ t know , this is the key to our proof of originality . but now that ai smashes a large number of works and then collages , all our previous methods are useless . obviously they are copied , but how can we defend our rights ? ” see also red velvet wendy celebrates 1st anniversary as a radio dj , members also leave messages to congratulate_work_look_feelingwu xinwei said that in reality , the image of a work may be copied but the style is not , but ai painting makes this copying possible : “ style is the creator ’ s label , and it is difficult to quantify that it is protected by law . ai painting is a fatal blow to this . . ” “ if ai painting has developed a lot , who will spend their whole life exploring ? ” mr. hu , who is engaged in art education , is also worried that if a “ not bad ” work can be generated in ten seconds , how many people are willing to spend it ? years of thinking , dozens of hours of painting ? however , there are also some relevant practitioners who hold the opposite view . they believe that ai painting can also become a tool for inspiration and auxiliary creation . mr. tang , who is engaged in film and television production , believes , “ if ai can inspire new inspirations and produce better works , why not do it ? ” liu jin also believes that there is no need to worry too much about ai painting : “ the progress of printing has not allowed the painting disappears , so why worry about ai painting ? ”\",\n",
       " \"/prnewswire/ -- imerit , a leading data labeling solutions company , announced today that sudeep george has joined the company as vice president of engineering . sudeep is an accomplished technology leader with over 18 years of experience in the computer vision industry . in his new role he will be located in bengaluru , india and focused on building the next generation of products and services for the artificial intelligence industry.throughout his career sudeep focused on building computational imaging products and solutions at the intersection of computer vision , imaging , edge computing and autonomous capabilities . as co-founder of tonbo imaging , he built a high-tech company focusing on imaging systems and created india 's first full-fledged electro-optics company . prior to tonbo , he led engineering teams at sarnoff corporation and worked at samsung r & d . `` we 're thrilled to have sudeep join imerit as we expand our global technology team and further develop our end-to-end data solutions . he will be instrumental in growing our technology products and services , and partnering with clients to create best-in-class solutions for addressing their unique data needs , '' said radha basu , founder and ceo of imerit . `` the opportunity at imerit is tremendous and i 'm excited to be part of a company that works with the world 's leaders in artificial intelligence and solving some of the most complex challenges in artificial intelligence today , '' said sudeep.in collaboration with deshpande foundation the company recently opened its tenth center of excellence for ai training data in hubballi , karnataka . with more than 4,500 employees , imerit 's global footprint includes centers across india , bhutan and the u.s.about imeritimerit is a leading data labeling solutions company providing high quality data across computer vision , natural language processing and content services that powers machine learning and artificial intelligence applications for large enterprises . imerit provides end-to-end data labeling services to fortune 500 companies in a wide array of industries including agricultural ai , autonomous vehicles , commerce , geospatial , government , financial services , medical ai and technology . imerit employs more than 4,500 full-time data annotation experts in bhutan , europe , india and the united states . raising $ 23.5 million in funding to date , imerit investors are cdc group , khosla impact , michael and susan dell foundation and omidyar network .\",\n",
       " \"/prnewswire/ -- dayton gastroenterology now offers patients enhanced colonoscopy screenings with the aid of gi genius™ intelligent endoscopy module . gi genius™ , the first ai system available in the us to assist in polyp detection during colonoscopy , uses artificial intelligence ( ai ) to help physicians detect polyps—a powerful new ally in the fight against colorectal cancer.vasu appalaneni , md , evp of clinical innovation for one gi and a senior physician with dayton gastroenterology , shares that the gi genius addition to dayton gastroenterology , a first for independent practices in dayton , stems from the physicians ' commitment to the continuous elevation of quality of care provided to gi patients throughout the greater dayton region . dr. appalaneni explained , `` we have partnered with medtronic and integrated the gi genius ai technology into our everyday colonoscopy procedures at all our locations to potentially increase the adenoma detection rate which decreases the incidence of colon cancer . `` jd keighley , one gi region vice president , explained , `` dayton gastroenterology , the largest gi practice in the region , is equipping our expert staff with the latest technologies and procedures to provide the best gi care possible , and we are very proud to offer this new technology to our patients . `` colorectal cancer is the third most common form of cancer diagnosed in the u.s. , with almost 150,000 new cases every year . medtronic 's gi genius system is a computer-assisted reading tool designed to aid endoscopists in detecting polyps and adenomas in real time during standard endoscopy examinations . studies have shown that ai-assisted colonoscopy can increase polyp detection rates , and every 1 % increase in adenoma detection rate reduces the risk of colorectal cancer by 3 % .\",\n",
       " \"hashtag trending feb.22nd-four-day work week worth more than money ; data centers used by alibaba , amazon , apple breached ; 60 countries including china agree to regulate military ai four-day work week worth more than money , hackers access data from asian data center operators used by alibaba , amazon , apple and more , and 60 countries , including china sign agreement to regulate military ai . the pilot for a four day work week that involved 3,000 employees across 61 british companies is now over and the results are worth thinking about , especially for companies struggling to attract and retain high value employees . the results of the trial consolidate the findings from previous research from 4 day week global , which found that 63 per cent of businesses surveyed found it easier to attract and retain employees after switching to a four-day work week . other staffers who participated also suggested that the four-day work week could be more attractive than money . 15 per cent said “ no amount of money ” would induce them to accept a five-day schedule over the four-day week to which they were now accustomed . according to documents reviewed by bloomberg , hackers have gotten hold of login credentials for data centers in asia used by some of the world ’ s biggest businesses including alibaba , amazon , apple , bmw , goldman sachs , huawei , microsoft and walmart . both data center operators said the rogue credentials didn ’ t pose a risk to clients ’ it systems or data but executives from four impacted us-based companies said the stolen credentials represented an unusual and serious danger , primarily because the customer-support websites control who is allowed to physically access the it equipment housed in the data centers . sixty countries including china have signed an agreement at the first global summit on responsible artificial intelligence in the military domain ( reaim ) to develop and use military ai in a responsible manner . they committed to abide by “ international legal obligations and not compromise “ international security , stability , and accountability , ” with their use of ai . hashtag trending goes to air five days a week with a daily newscast and we have a special weekend edition with an interview featuring an expert in some aspect of technology that is making the news . jim lovehttp : //www.itworldcanada.comi 've been in it and business for over 30 years . i worked my way up , literally from the mail room and i 've done every job from mail clerk to ceo . today i 'm cio of a great company - it world canada - canada 's leading ict publisher . hashtag trending feb.14- layoffs hamper innovation , ai cloning technology shows president biden read transphobic text and data brokers selling mental health records\",\n",
       " \"'unapproved voice cloning is a slippery slope ” : the use of deepfakes in anthony bourdain ’ s docu roadrunner has drawn criticism amid ethical concerns about the use of this technology . the revelation that a documentary filmmaker used voice-cloning software to make the late chef anthony bourdain say words he never spoke has drawn criticism amid ethical concerns about the use of the powerful technology . the movie roadrunner : a film about anthony bourdain appeared in cinemas friday and mostly features real footage of the beloved celebrity chef and globe-trotting television host before he died in 2018. but its director , morgan neville , told the new yorker that a snippet of dialogue was created using artificial intelligence technology . that ’ s renewed a debate about the future of voice-cloning technology , not just in the entertainment world but in politics and a fast-growing commercial sector dedicated to transforming text into realistic-sounding human speech . “ unapproved voice cloning is a slippery slope , ” said andrew mason , the founder and ceo of voice generator descript , in a blog post on friday . “ as soon as you get into a world where you ’ re making subjective judgment calls about whether specific cases can be ethical , it won ’ t be long before anything goes . ” angry and uncomfortable reactions to the voice cloning in the bourdain case reflect expectations and issues of disclosure and consent , said sam gregory , program director at witness , a nonprofit working on using video technology for human rights . obtaining consent and disclosing the technowizardry at work would have been appropriate , he said . instead , viewers were stunned — first by the fact of the audio fakery , then by the director ’ s seeming dismissal of any ethical questions — and expressed their displeasure online . “ with the blessing of his estate and literary agent we used ai technology , ” neville said in a written statement . “ it was a modern storytelling technique that i used in a few places where i thought it was important to make tony ’ s words come alive . ” neville also told gq magazine that he got the approval of bourdain ’ s widow and literary executor . the chef ’ s wife , ottavia busia , responded by tweet : “ i certainly was not the one who said tony would have been cool with that . ” many of these voice cloning companies prominently feature an ethics policy on their website that explains the terms of use . of nearly a dozen firms contacted by the associated press , many said they didn ’ t recreate bourdain ’ s voice and wouldn ’ t have if asked . others didn ’ t respond . “ we have pretty strong polices around what can be done on our platform , ” said zohaib ahmed , founder and ceo of resemble ai , a toronto company that sells a custom ai voice generator service . “ when you ’ re creating a voice clone , it requires consent from whoever ’ s voice it is . ” ahmed said the rare occasions where he ’ s allowed some posthumous voice cloning were for academic research , including a project working with the voice of winston churchill , who died in 1965 . ahmed said a more common commercial use is to edit a tv ad recorded by real voice actors and then customize it to a region by adding a local reference . it ’ s also used to dub anime movies and other videos , by taking a voice in one language and making it speak a different language , he said . just seconds or minutes of recorded human speech can help teach an ai system to generate its own synthetic speech , though getting it to capture the clarity and rhythm of anthony bourdain ’ s voice probably took a lot more training , said rupal patel , a professor at northeastern university who runs another voice-generating company , vocalid , that focuses on customer service chatbots . neville is an acclaimed documentarian who also directed the fred rogers portrait won ’ t you be my neighbor ? and the oscar-winning 20 feet from stardom . he began making his latest movie in 2019 , more than a year after bourdain ’ s death by suicide in june 2018 . petrol and diesel price today : in delhi , petrol prices remained at rs 101.84 a litre while the rate of diesel was at rs 89.87. check latest rates in your city . latest petrol and diesel prices at firstpost.com america : the motion picture on netflix is an aggressively inane cartoon that flips the bird to a strain of patriotism that insists that men who profited from slavery were sober-minded heroes whose vision of democracy remains flawless . the authenticity portrayed in films like luca only happens when people from the communities represented onscreen are also working behind the camera .\",\n",
       " 'a new study published in science advances by european scientists working in optoelectronics have created a biocompatible artificial intelligence ( ai ) platform that may enable a novel way for the early detection , monitoring , and treatment of diseases and medical conditions . in current biocompatible organic materials for electronics , namely organic electrochemical transistors ( oects ) , the processing of the information is somewhat inadequate . in efforts to solve this , the scientists took a brain-inspired framework for computation . the researchers created a biocompatible hardware-based artificial neural network ( ann ) based on organic electrochemical transistors . “ in this work , we build nonlinear , dendritic networks of oects and use them for information processing on biosignals , demonstrating real-time classification in a biocompatible , hardware neural network , ” wrote the researchers . “ the results of this study introduce a previously unexplored paradigm for biocompatible computational platforms and may enable development of ultralow–power consumption hardware-based artificial neural networks capable of interacting with body fluids and biological tissues , ” the scientist reported.article continues after advertisement by combining the innovative technologies of ai machine learning with optoelectronics , scientists have taken a step towards novel diagnostics and therapeutics that may extend human longevity and overall quality of life in the future .',\n",
       " 'washington ( ap ) president joe biden met tuesday with his advisory board on science and technology about the risks and opportunities that rapid advances in artificial intelligence development pose to individual users and national security . ai can help address some of the toughest challenges like disease and climate change , but it also needs to address potential risks to society , the economy and national security , biden told the group . rice field . the white house said the democratic president will use the ai \\u200b\\u200bconference to discuss the importance of protecting rights and security to ensure responsible innovation and adequate protection , to protect children and protect data by tech companies , the white house said . he reiterated his call for congress to pass legislation to curb collection . artificial intelligence has come to the forefront of national and global conversations after the release of the popular chatgpt ai chatbot . this has started a race among tech giants to launch similar tools . it looks like a human job . italy temporarily blocked chatgpt last week over data privacy concerns . and european union lawmakers are negotiating with new regulators to limit risky ai products . so far , the us has taken a different approach . the biden administration last year announced a broad set of goals aimed at avoiding the harm caused by the rise of ai systems , including guidelines on how to protect people ’ s personal data and limit surveillance . the ai \\u200b\\u200bbill of rights blueprint did not set out any specific enforcement actions , but instead served as a white house call to action for the u.s. government to protect digital and civil rights in an ai-powered world . was intended .',\n",
       " 'in this report , the global artificial intelligence in computer networks market is valued at usd xx million in 2017 and is expected to reach usd xx million by the end of 2025 , growing at a cagr of xx % between 20 . global artificial intelligence in computer networks market has been broken down by major regions , with complete market estimates on the basis of products/applications on a regional basis . browse full research report at https : //www.crystalmarketreport.com/global-artificial-intelligence-in-computer-networks-market-size-status-and-forecast-2019-2025 in general , the application of artificial intelligence in computer network technology mainly includes network management , security management , and artificial intelligence.therefore , artificial intelligence is used in computer network technology to meet data processing requirements . in 2018 , the global artificial intelligence in computer networks market size was xx million us $ and it is expected to reach xx million us $ by the end of 2025 , with a cagr of xx % during 2019-2025 . this report focuses on the global artificial intelligence in computer networks status , future forecast , growth opportunity , key market and key players . the study objectives are to present the artificial intelligence in computer networks development in north america , europe , china , japan , southeast asia , india and central & south america . browse full research report at https : //www.crystalmarketreport.com/global-artificial-intelligence-in-computer-networks-market-size-status-and-forecast-2019-2025 in-depth quantitative information on key regional global artificial intelligence in computer networks markets including north america , europe , mea and asia pacific',\n",
       " \"london ( ap ) — the company behind chatgpt will propose measures to resolve data privacy concerns that sparked a temporary italian ban on the artificial intelligence chatbot , regulators said thursday . in a video call late wednesday between the watchdog 's commissioners and openai executives including ceo sam altman , the company promised to set out measures to address the concerns . those remedies have not been detailed . the italian watchdog said it did n't want to hamper ai 's development but stressed to openai the importance of complying with the 27-nation eu 's stringent privacy rules . the regulators imposed the ban after some users ' messages and payment information were exposed to others . they also questioned whether there 's a legal basis for openai to collect massive amounts of data used to train chatgpt 's algorithms and raised concerns the system could sometimes generate false information about individuals . so-called generative ai technology like chatgpt is “ trained ” on huge pools of data , including digital books and online writings , and able to generate text that mimics human writing styles . ireland 's data protection commission said it 's “ following up with the italian regulator to understand the basis for their action and we will coordinate with all eu data protection authorities in relation to this matter . ” in an apparent response to the concerns , openai published a blog post wednesday outlining its approach to ai safety . the company said it works to remove personal information from training data where feasible , fine-tune its models to reject requests for personal information of private individuals , and acts on requests to delete personal information from its systems .\",\n",
       " \", 8:36 p.m.·3 min readwashington ( ap ) — president joe biden said tuesday it remains to be seen if artificial intelligence is dangerous , but that he believes technology companies must ensure their products are safe before releasing them to the public.biden met with his council of advisers on science and technology about the risks and opportunities that rapid advancements in artificial intelligence pose for individual users and national security. “ ai can help deal with some very difficult challenges like disease and climate change , but it also has to address the potential risks to our society , to our economy , to our national security , ” biden told the group , which includes academics as well as executives from microsoft and google.artificial intelligence burst to the forefront in the national and global conversation in recent months after the release of the popular chatgpt ai chatbot , which helped spark a race among tech giants to unveil similar tools , while raising ethical and societal concerns about technology that can generate convincing prose or imagery that looks like it 's the work of humans.while tech companies should always be responsible for the safety of their products , biden 's reminder reflects something new — the emergence of easy-to-use ai tools that can generate manipulative content and realistic-looking synthetic media known as deepfakes , said rebecca finley , ceo of the industry-backed partnership on ai.the white house said the democratic president was using the ai meeting to “ discuss the importance of protecting rights and safety to ensure responsible innovation and appropriate safeguards ” and to reiterate his call for congress to pass legislation to protect children and curtail data collection by technology companies.italy last week temporarily blocked chatgpt over data privacy concerns , and european union lawmakers have been negotiating the passage of new rules to limit high-risk ai products across the 27-nation bloc.by contrast , “ the u.s. has had more a laissez-faire approach to the commercial development of ai , ” said russell wald , managing director of policy and society at the stanford institute for human-centered artificial intelligence.story continuesbiden 's tuesday remarks wo n't likely change that , but biden “ is setting the stage for a national dialogue on the topic by elevating attention to ai , which is desperately needed , ” wald said.the biden administration last year unveiled a set of far-reaching goals aimed at averting harms caused by the rise of ai systems , including guidelines for how to protect people ’ s personal data and limit surveillance.the blueprint for an ai bill of rights notably did not set out specific enforcement actions , but instead was intended as a call to action for the u.s. government to safeguard digital and civil rights in an ai-fueled world.biden 's council , known as pcast , is composed of science , engineering , technology and medical experts and is co-chaired by the cabinet-ranked director of the white house office of science and technology policy , arati prabhakar.asked if ai is dangerous , biden said tuesday , “ it remains to be seen . could be. ” —————ap writers chris megerian and matt o'brien contributed to this report.zeke miller , the associated press\"]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mode of the items\n",
    "from collections import Counter\n",
    "\n",
    "sentiments\n",
    "\n",
    "counter = Counter(sentiments)\n",
    "most_common_item = counter.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NEUTRAL'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning done in 1.1734158992767334 seconds\n",
      "Tokenization done in 0.011795759201049805 seconds\n",
      "Frequency Calculation Time: 0.027017831802368164 seconds\n",
      "Stop Words Creation Time: 0.0022242069244384766 seconds\n",
      "Stop Words Filtering Time: 0.11146306991577148 seconds\n",
      "Dictionary Creation Time: 0.11047101020812988 seconds\n",
      "Corpus Creation Time: 0.06594204902648926 seconds\n",
      "CPU times: user 1.46 s, sys: 44.7 ms, total: 1.5 s\n",
      "Wall time: 1.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the number of topics\n",
    "num_topics = 10\n",
    "\n",
    "# Preprocess your news articles and create a list of documents\n",
    "# Each document should be a list of tokens (words) representing an article\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "doc_complete = df_news_final_project.head(500)['text_clean'].values.tolist()\n",
    "\n",
    "def clean(doc):\n",
    "    if type(doc) != str:\n",
    "        return ''\n",
    "    doc = ' '.join([i for i in doc.split() if len(i) < 20])\n",
    "    doc = ' '.join([i for i in doc.split() if len(i) > 1])\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in word_tokenize(punc_free))\n",
    "    normalized = normalized.replace('’', '')\n",
    "    normalized = normalized.replace('“', '')\n",
    "    normalized = normalized.replace('”', '')\n",
    "    return normalized\n",
    "\n",
    "# Preprocess your documents\n",
    "start_time = time.time()\n",
    "cleaned_documents = [clean(doc) for doc in doc_complete]\n",
    "clean_time = time.time() - start_time\n",
    "print(\"Cleaning done in\", clean_time, \"seconds\")\n",
    "\n",
    "# Split the cleaned documents into tokens\n",
    "start_time = time.time()\n",
    "tokenized_documents = [doc.split() for doc in cleaned_documents]\n",
    "tok_time = time.time() - start_time\n",
    "print(\"Tokenization done in\", tok_time, \"seconds\")\n",
    "\n",
    "# Calculate word frequencies\n",
    "start_time = time.time()\n",
    "word_freq = defaultdict(int)\n",
    "for document in tokenized_documents:\n",
    "    for word in document:\n",
    "        word_freq[word] += 1\n",
    "freq_time = time.time() - start_time\n",
    "print(\"Frequency Calculation Time:\", freq_time, \"seconds\")\n",
    "\n",
    "# Set the threshold for popular words\n",
    "threshold = 0.5  # Adjust this value according to your needs\n",
    "\n",
    "# Create a list of stop words based on the threshold\n",
    "start_time = time.time()\n",
    "stop_words = [word for word, freq in word_freq.items() if freq / len(tokenized_documents) > threshold]\n",
    "stop_words_time = time.time() - start_time\n",
    "print(\"Stop Words Creation Time:\", stop_words_time, \"seconds\")\n",
    "\n",
    "# Filter out stop words from the tokenized documents\n",
    "start_time = time.time()\n",
    "filtered_documents = [[word for word in document if word not in stop_words] for document in tokenized_documents]\n",
    "filtering_time = time.time() - start_time\n",
    "print(\"Stop Words Filtering Time:\", filtering_time, \"seconds\")\n",
    "\n",
    "# Create a dictionary from the preprocessed and filtered documents\n",
    "start_time = time.time()\n",
    "dictionary = corpora.Dictionary(filtered_documents)\n",
    "dictionary_creation_time = time.time() - start_time\n",
    "print(\"Dictionary Creation Time:\", dictionary_creation_time, \"seconds\")\n",
    "\n",
    "# Convert the dictionary into a bag-of-words representation\n",
    "start_time = time.time()\n",
    "corpus = [dictionary.doc2bow(doc) for doc in filtered_documents]\n",
    "corpus_creation_time = time.time() - start_time\n",
    "print(\"Corpus Creation Time:\", corpus_creation_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_documents = [' '.join(item) for item in filtered_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lang: en\n",
      "preprocessing texts...\n",
      "fitting model...\n",
      "iteration: 1 of max_iter: 5\n",
      "iteration: 2 of max_iter: 5\n",
      "iteration: 3 of max_iter: 5\n",
      "iteration: 4 of max_iter: 5\n",
      "iteration: 5 of max_iter: 5\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tm = ktrain.text.get_topic_model(\n",
    "    texts=all_documents,\n",
    "    n_topics=num_topics, \n",
    "    n_features=10000,\n",
    "    min_df=5,\n",
    "    max_df=0.5,\n",
    "    stop_words='english',\n",
    "    model_type='lda',\n",
    "    lda_max_iter=5,\n",
    "    verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0 | patient medical clinical care cookie disease healthcare insurance lender imaging\n",
      "topic 1 | communication project analytics guided cloud regulated course ibm transaction government\n",
      "topic 2 | player forecast trend region provides study opportunity healthcare supply strategy\n",
      "topic 3 | creative asset investment management insight campaign rain brand marketer morgan\n",
      "topic 4 | tesla musk elon day alto ceo bot selfdriving great event\n",
      "topic 5 | trump say tech investment organization offer region release way current\n",
      "topic 6 | organization release security cloud digital capability today press enterprise support\n",
      "topic 7 | study medical microsoft problem program bank today digital center financial\n",
      "topic 8 | openai leading big microsoft africa organization analytics sector ceo access\n",
      "topic 9 | openai say google microsoft search news tech way video question\n"
     ]
    }
   ],
   "source": [
    "tm.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ktrain.text.zsl import ZeroShotClassifier\n",
    "\n",
    "zsl = ZeroShotClassifier()\n",
    "labels=['healthcare', 'self-driving', 'brand strategy', 'technology']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"allison longyael bensoussan , md , is part of the usf health 's department of otolaryngology - head & neck surgery . she 's leading an effort to collect voice data that can be used to diagnose illnesses . the national institutes of health is funding a massive research project to collect voice data and develop an ai that could diagnose people based on their speech . everything from your vocal cord vibrations to breathing patterns when you speak offers potential information about your health , says laryngologist dr. yael bensoussan , the director of the university of south florida 's health voice center and a leader on the study . `` we asked experts : well , if you close your eyes when a patient comes in , just by listening to their voice , can you have an idea of the diagnosis they have ? '' bensoussan says . `` and that 's where we got all our information . '' someone who speaks low and slowly might have parkinson 's disease . slurring is a sign of a stroke . scientists could even diagnose depression or cancer . the team will start by collecting the voices of people with conditions in five areas : neurological disorders , voice disorders , mood disorders , respiratory disorders and pediatric disorders like autism and speech delays . the project is part of the nih 's bridge to ai program , which launched over a year ago with more than $ 100 million in funding from the federal government , with the goal of creating large-scale health care databases for precision medicine . this is n't the first time researchers have used ai to study human voices , but it 's the first time data will be collected on this level — the project is a collaboration between usf , cornell and 10 other institutions . the ultimate goal is an app that could help bridge access to rural or underserved communities , by helping general practitioners refer patients to specialists . long term , iphones or alexa could detect changes in your voice , such as a cough , and advise you to seek medical attention . to get there , researchers have to start by amassing data , since the ai can only get as good as the database it 's learning from . by the end of the four years , they hope to collect about 30,000 voices , with data on other biomarkers — like clinical data and genetic information — to match . `` let 's say you donate your voice to our project , '' says yael bensoussan . `` who does the voice belong to ? what are we allowed to do with it ? what are researchers allowed to do with it ? can it be commercialized ? ''\""
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news_final_project['text_clean'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UltraSight Receives CE Mark for Novel Cardiac AI Technology'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news_final_project['title'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'technology'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = df_news_final_project['title'].iloc[1]\n",
    "pred = zsl.predict(doc, labels=labels, include_labels=True)\n",
    "\n",
    "# find closet topic\n",
    "max(pred, key=lambda x: x[1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the campaign ai now recruits more intelligently, and transfers units to threatened theaters actively. capital city defense behavior, defensive operations, weapons production and distribution among the troops are also among the upgrades. along comes also a long list of bug fixes, balancing, ui and other improvements.'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Classify new articles based on topics\n",
    "new_articles = [\n",
    "    \"AI's applications in the healthcare industry\",\n",
    "    \"How AI is revolutionizing the finance sector\",\n",
    "    \"The impact of AI on the retail industry\",\n",
    "    \"Transportation and AI: A futuristic combination\"\n",
    "]\n",
    "\n",
    "preprocessed_new_articles = preprocessor.preprocess_text(new_articles)\n",
    "\n",
    "predicted_topics = model.predict(preprocessed_new_articles)\n",
    "\n",
    "for article, topic_id in zip(new_articles, predicted_topics):\n",
    "    print(f\"Article: {article}\")\n",
    "    print(f\"Predicted Topic ID: {topic_id}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news_final_project"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m108"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
